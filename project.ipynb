{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rnd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = open('keptcolumns.txt', 'r')\n",
    "keptcolumns = fi.read().rstrip('\\n').split(', ')\n",
    "data = pd.read_csv('epi_r.csv', usecols = keptcolumns)\n",
    "data = data[pd.notnull(data['calories'])]\n",
    "data = data.values\n",
    "h_val = data[:, 0:5]\n",
    "ingred = data[:, 5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dat(b_vec):\n",
    "    one_cnt = np.sum(b_vec == 1)\n",
    "    one_idx = np.where(b_vec == 1)\n",
    "\n",
    "    n = one_cnt // 2\n",
    "\n",
    "    x = np.zeros((n, b_vec.shape[0]))\n",
    "    y = np.zeros((n, b_vec.shape[0]))\n",
    "\n",
    "    for i in range(n):\n",
    "        x_size = rnd.randint(n, one_cnt - 1)\n",
    "        x_one_idx = np.random.choice(one_idx[0], x_size, replace = False)\n",
    "        y_one_idx = np.setdiff1d(one_idx[0], x_one_idx)\n",
    "        x[i, x_one_idx] = 1\n",
    "        y[i, y_one_idx] = 1\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27486, 308)\n",
      "(27486, 308)\n"
     ]
    }
   ],
   "source": [
    "x_aug = np.empty((0, ingred.shape[1]))\n",
    "y_aug = np.empty((0, ingred.shape[1]))\n",
    "for i in range(ingred.shape[0]):\n",
    "    x, y = gen_dat(ingred[i])\n",
    "    x_aug = np.append(x_aug, x, axis = 0)\n",
    "    y_aug = np.append(y_aug, y, axis = 0)\n",
    "print(x_aug.shape)\n",
    "print(y_aug.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition = x_aug.shape[0] * 8 // 10\n",
    "# x_aug_train = x_aug[0:partition, :]\n",
    "# y_aug_train = y_aug[0:partition, :]\n",
    "# x_aug_test = x_aug[partition:, :]\n",
    "# y_aug_test = y_aug[partition:, :]\n",
    "x_aug_train = x_aug\n",
    "y_aug_train = y_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_85 (Dense)             (None, 200)               61800     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_71 (LeakyReLU)   (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_72 (LeakyReLU)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_73 (LeakyReLU)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_74 (LeakyReLU)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_75 (LeakyReLU)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 308)               31108     \n",
      "=================================================================\n",
      "Total params: 143,308\n",
      "Trainable params: 143,308\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([Dense(200, input_shape=(308,)),\n",
    "                                    LeakyReLU(),\n",
    "                                    \n",
    "                                    Dense(100),\n",
    "                                    LeakyReLU(),\n",
    "                                    \n",
    "                                    Dense(100),\n",
    "                                    LeakyReLU(),\n",
    "                                                                                                            \n",
    "                                    Dense(100),\n",
    "                                    LeakyReLU(),\n",
    "                                    \n",
    "                                    Dense(100),\n",
    "                                    LeakyReLU(),\n",
    "                                    \n",
    "                                    Dense(308, activation='sigmoid')\n",
    "                                   ])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "27486/27486 [==============================] - 2s 58us/step - loss: 0.0869\n",
      "Epoch 2/5\n",
      "27486/27486 [==============================] - 1s 42us/step - loss: 0.0324\n",
      "Epoch 3/5\n",
      "27486/27486 [==============================] - 1s 42us/step - loss: 0.0324\n",
      "Epoch 4/5\n",
      "27486/27486 [==============================] - 1s 42us/step - loss: 0.0318\n",
      "Epoch 5/5\n",
      "27486/27486 [==============================] - 1s 42us/step - loss: 0.0309\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_aug_train, y_aug_train, epochs=5, batch_size=128)\n",
    "# loss_and_metrics = model.evaluate(x_aug_test, y_aug_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['potato' 'bell pepper' 'salad' 'garlic' 'soup/stew' 'pepper' 'herb'\n",
      " 'pasta' 'onion' 'tomato']\n"
     ]
    }
   ],
   "source": [
    "x_aug_test = np.zeros((1, ingred.shape[1]))\n",
    "idx = [8, 46, 219, 289]\n",
    "x_aug_test[0][idx] = 1\n",
    "y_aug_test = model.predict(x_aug_test, batch_size=128)\n",
    "idx = np.argsort(y_aug_test[0])\n",
    "idx = idx[-10:]\n",
    "idx = idx + 5\n",
    "print(keptcolumns[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  8  46 219 230 289]]\n",
      "[['asparagus' 'capers' 'potato' 'radish' 'tuna']]\n"
     ]
    }
   ],
   "source": [
    "x = np.array(np.where(ingred[15] == 1)) + 5\n",
    "print(x - 5)\n",
    "print(keptcolumns[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['rating', 'calories', 'protein', 'fat', 'sodium', 'almond',\n",
       "       'anchovy', 'apple', 'apple juice', 'apricot', 'artichoke',\n",
       "       'arugula', 'asian pear', 'asparagus', 'avocado', 'bacon', 'banana',\n",
       "       'barley', 'basil', 'bass', 'bean', 'beef', 'beef rib',\n",
       "       'beef shank', 'beef tenderloin', 'beer', 'beet', 'bell pepper',\n",
       "       'berry', 'biscuit', 'blackberry', 'blue cheese', 'blueberry',\n",
       "       'bourbon', 'brandy', 'bread', 'breadcrumbs', 'brie', 'brine',\n",
       "       'brisket', 'broccoli', 'brown rice', 'brownie', 'brussel sprout',\n",
       "       'butter', 'buttermilk', 'butternut squash', 'butterscotch/caramel',\n",
       "       'cabbage', 'candy', 'cantaloupe', 'capers', 'caraway', 'cardamom',\n",
       "       'carrot', 'cashew', 'casserole/gratin', 'cauliflower', 'caviar',\n",
       "       'celery', 'chard', 'cheddar', 'cheese', 'cherry', 'chestnut',\n",
       "       'chicken', 'chickpea', 'chile pepper', 'chili', 'chocolate',\n",
       "       'cilantro', 'cinnamon', 'citrus', 'clam', 'clove',\n",
       "       'cobbler/crumble', 'coconut', 'cod', 'coffee', 'collard greens',\n",
       "       'condiment', 'condiment/spread', 'cookie', 'coriander', 'corn',\n",
       "       'cornmeal', 'cottage cheese', 'couscous', 'crab', 'cranberry',\n",
       "       'cranberry sauce', 'cream cheese', 'cucumber', 'cumin', 'currant',\n",
       "       'custard', 'dill', 'dried fruit', 'duck', 'egg', 'egg nog',\n",
       "       'eggplant', 'endive', 'escarole', 'fennel', 'feta', 'fig', 'fish',\n",
       "       'flat bread', 'fontina', 'frittata', 'fruit', 'fruit juice',\n",
       "       'garlic', 'gin', 'ginger', 'goat cheese', 'goose', 'gouda',\n",
       "       'grains', 'granola', 'grape', 'grapefruit', 'green bean',\n",
       "       'green onion/scallion', 'ground beef', 'ground lamb', 'guava',\n",
       "       'ham', 'hazelnut', 'herb', 'hominy/cornmeal/masa', 'honey',\n",
       "       'honeydew', 'horseradish', 'hot pepper', 'hummus', 'ice cream',\n",
       "       'iced coffee', 'iced tea', 'jalapeño', 'jam or jelly', 'kale',\n",
       "       'kiwi', 'kumquat', 'lamb', 'lamb chop', 'lamb shank', 'lasagna',\n",
       "       'legume', 'lemon', 'lemon juice', 'lemongrass', 'lentil',\n",
       "       'lettuce', 'lima bean', 'lime', 'lime juice', 'lingonberry',\n",
       "       'lobster', 'lychee', 'macadamia nut', 'macaroni and cheese',\n",
       "       'mango', 'maple syrup', 'margarita', 'marscarpone', 'mayonnaise',\n",
       "       'meat', 'meatball', 'meatloaf', 'melon', 'milk/cream', 'mint',\n",
       "       'mozzarella', 'mushroom', 'mussel', 'mustard', 'mustard greens',\n",
       "       'nectarine', 'noodle', 'nut', 'nutmeg', 'oat', 'oatmeal',\n",
       "       'octopus', 'okra', 'olive', 'onion', 'orange', 'orange juice',\n",
       "       'oregano', 'oyster', 'pancake', 'papaya', 'paprika', 'parmesan',\n",
       "       'parsley', 'parsnip', 'passion fruit', 'pasta', 'pea', 'peach',\n",
       "       'peanut', 'peanut butter', 'pear', 'pecan', 'pepper', 'persimmon',\n",
       "       'pickles', 'pine nut', 'pineapple', 'pistachio', 'plantain',\n",
       "       'plum', 'poblano', 'pomegranate', 'pomegranate juice', 'poppy',\n",
       "       'pork', 'pork chop', 'pork rib', 'pork tenderloin', 'pot pie',\n",
       "       'potato', 'potato salad', 'poultry sausage', 'prosciutto',\n",
       "       'pumpkin', 'quail', 'quiche', 'quince', 'rabbit', 'rack of lamb',\n",
       "       'radicchio', 'radish', 'raisin', 'raspberry', 'red wine',\n",
       "       'rhubarb', 'rice', 'ricotta', 'root vegetable', 'rosemary', 'rum',\n",
       "       'rutabaga', 'rye', 'saffron', 'sake', 'salad', 'salad dressing',\n",
       "       'salmon', 'salsa', 'sardine', 'sauce', 'sausage', 'scallop',\n",
       "       'seed', 'sesame', 'sesame oil', 'shallot', 'shellfish', 'shrimp',\n",
       "       'snapper', 'soup/stew', 'sour cream', 'sourdough', 'soy',\n",
       "       'soy sauce', 'sparkling wine', 'spinach', 'spirit', 'squash',\n",
       "       'squid', 'strawberry', 'sugar snap pea', 'sweet potato/yam',\n",
       "       'swiss cheese', 'swordfish', 'tamarind', 'tangerine', 'tapioca',\n",
       "       'tarragon', 'tea', 'tequila', 'thyme', 'tilapia', 'tofu',\n",
       "       'tomatillo', 'tomato', 'tortillas', 'tree nut', 'tropical fruit',\n",
       "       'trout', 'tuna', 'turnip', 'vanilla', 'veal', 'venison', 'vinegar',\n",
       "       'vodka', 'walnut', 'wasabi', 'watercress', 'watermelon', 'whiskey',\n",
       "       'white wine', 'wild rice', 'yellow squash', 'yogurt', 'yuca',\n",
       "       'zucchini', 'turkey'], dtype='<U20')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keptcolumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5. 421.  10.  33. 383.]\n"
     ]
    }
   ],
   "source": [
    "print(h_val[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
